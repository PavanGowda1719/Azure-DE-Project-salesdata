{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ca9d0e-af50-4156-8ad3-7547294a1d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##***creating catalog***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a8619f-db2c-4327-b934-eaf600f7308b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "create catalog ansh_lamba;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e167d1-7ead-4692-bc58-44b0741fc466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Set the catalog context\n",
    "USE CATALOG ansh_lamba;\n",
    "\n",
    "-- Create schema under the specified catalog\n",
    "CREATE SCHEMA salesdata;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5dd416b-6b55-4456-a4e2-f8b6c6294b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show schemas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d0649f-8c0e-41e5-b69e-e84ba23e8bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--  creating managed table\n",
    "\n",
    "create table salesdata.sales_data (id int, name varchar(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2222bc46-1070-4eb2-9f1d-94726a6a7587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Triggering email when scema changes\n",
    "### Using Logic APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e72830e-d47f-46aa-9c72-f345b2b060a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Reference schema (stored schema)\n",
    "reference_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load new data\n",
    "df = spark.table('ansh_lamba.salesdata.silver')\n",
    "\n",
    "new_schema = df.schema\n",
    "\n",
    "def schemas_are_equal(schema1, schema2):\n",
    "    return schema1.simpleString() == schema2.simpleString()\n",
    "\n",
    "# Detect schema changes\n",
    "if not schemas_are_equal(reference_schema, new_schema):\n",
    "    print(\"Schema mismatch detected!\")\n",
    "\n",
    "    # Logic App trigger using Webhook URL\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    # Webhook URL of Logic App\n",
    "    webhook_url = \"https://prod-07.southindia.logic.azure.com:443/workflows/5b91112106bc4340855f9201705342f2/triggers/When_a_HTTP_request_is_received/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2FWhen_a_HTTP_request_is_received%2Frun&sv=1.0&sig=sRusFLDIpAZ87XQnupQ76PDLALE1e4cyBFCrVy0cxAA\"\n",
    "\n",
    "    # Example payload to send to Logic App\n",
    "    payload = {\n",
    "    \"schema_info\": {\n",
    "        \"old_schema\": reference_schema.simpleString(),\n",
    "        \"new_schema\": new_schema.simpleString()\n",
    "    },\n",
    "    \"message\": \"Schema has changed! Please review. cjbsjvhdsjvhsdmvndsmvndvnsmv zvndzbv mdzv bmzdbv \",\n",
    "    \"to\": \"pavangowda1706@outlook.com\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(webhook_url, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))\n",
    "\n",
    "    if response.status_code == 200 | 202 :\n",
    "        print(\"Notification sent successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to send notification. Status code: {response.status_code}\")\n",
    "\n",
    "else:\n",
    "    print(\"Schema is up to date!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "442c5420-ac7d-410d-8ffa-53cee8d6dc35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Handling schema evaluation**\n",
    "\n",
    "**NOTE** - **In delta table we can handled by merge schema**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34d682c-dccc-4a5c-baf5-0805213c3765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def adjust_schema(dataframe, reference_schema):\n",
    "    for field in reference_schema.fields:\n",
    "        if field.name not in dataframe.columns:\n",
    "            dataframe = dataframe.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Remove extra columns and reorder\n",
    "    dataframe = dataframe.select([field.name for field in reference_schema.fields if field.name in dataframe.columns])\n",
    "    return dataframe\n",
    "\n",
    "# Adjust schema of new_data\n",
    "new_data = adjust_schema(df, reference_schema)\n",
    "\n",
    "## we can derive the dataframe baased on need\n",
    "display(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e4c04b-e28b-4529-8cb4-84877d6c743c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8256953177678713,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "first_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
